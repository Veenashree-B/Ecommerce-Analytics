{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1638eb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: LOAD & EXPLORE DATA\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xa0 in position 2944: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/raw/superstore.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä Dataset Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìã Column Names & Types:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:574\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:663\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._get_header\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xa0 in position 2944: invalid start byte"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=================================================================\n",
    "PHASE 2 & 3: DATA CLEANING & FEATURE ENGINEERING\n",
    "=================================================================\n",
    "This notebook handles:\n",
    "1. Loading and exploring the raw data\n",
    "2. Cleaning & handling missing values (logical decisions documented)\n",
    "3. Removing duplicates & anomalies\n",
    "4. Creating engineered features for analysis\n",
    "=================================================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: LOAD & EXPLORE DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load data with proper encoding handling\n",
    "df = pd.read_csv('../data/raw/superstore.csv', encoding='latin-1')\n",
    "\n",
    "print(f\"\\nüìä Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nüìã Column Names & Types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nüîç First few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nüìà Data Info:\")\n",
    "print(df.info())\n",
    "print(f\"\\n‚ùì Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0] if missing.sum() > 0 else \"No missing values\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: DATA CLEANING - DECISION LOG\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a decision log\n",
    "decisions = []\n",
    "\n",
    "# ===== DECISION 1: Handle Missing Values =====\n",
    "decision = \"MISSING VALUES: No missing values found in dataset\"\n",
    "print(f\"\\n‚úÖ {decision}\")\n",
    "decisions.append(decision)\n",
    "\n",
    "# ===== DECISION 2: Remove Duplicates =====\n",
    "duplicates_before = df.duplicated().sum()\n",
    "df = df.drop_duplicates()\n",
    "duplicates_after = df.duplicated().sum()\n",
    "decision = f\"DUPLICATES: Removed {duplicates_before} duplicate rows\"\n",
    "print(f\"‚úÖ {decision}\")\n",
    "decisions.append(decision)\n",
    "\n",
    "# ===== DECISION 3: Parse Date Columns =====\n",
    "print(\"\\nüîÑ Parsing date columns...\")\n",
    "# Identify date columns (adjust based on your actual data)\n",
    "date_columns = [col for col in df.columns if 'date' in col.lower() or 'order' in col.lower()]\n",
    "print(f\"Detected date columns: {date_columns}\")\n",
    "\n",
    "# Parse dates\n",
    "for col in df.columns:\n",
    "    if 'date' in col.lower():\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        decision = f\"DATE PARSING: Converted '{col}' to datetime\"\n",
    "        print(f\"‚úÖ {decision}\")\n",
    "        decisions.append(decision)\n",
    "\n",
    "# ===== DECISION 4: Handle Inconsistent Categories =====\n",
    "print(\"\\nüè∑Ô∏è Checking categorical columns...\")\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "for col in categorical_cols:\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"  {col}: {unique_count} unique values\")\n",
    "    # Strip whitespace from all object columns\n",
    "    df[col] = df[col].str.strip() if df[col].dtype == 'object' else df[col]\n",
    "\n",
    "decision = \"CATEGORIES: Stripped whitespace from all categorical columns\"\n",
    "print(f\"‚úÖ {decision}\")\n",
    "decisions.append(decision)\n",
    "\n",
    "# ===== DECISION 5: Handle Negative/Invalid Values =====\n",
    "print(\"\\n‚ö†Ô∏è Checking for invalid/negative values...\")\n",
    "\n",
    "# Check for negative profit\n",
    "negative_profit = (df['Profit'] < 0).sum() if 'Profit' in df.columns else 0\n",
    "if negative_profit > 0:\n",
    "    print(f\"  Found {negative_profit} rows with negative profit (KEPT - some products DO lose money)\")\n",
    "    decision = f\"NEGATIVE PROFIT: Kept {negative_profit} negative profit rows (domain logic: some orders lose money)\"\n",
    "    decisions.append(decision)\n",
    "\n",
    "# Check for negative quantity\n",
    "negative_qty = (df['Quantity'] < 0).sum() if 'Quantity' in df.columns else 0\n",
    "if negative_qty > 0:\n",
    "    print(f\"  Found {negative_qty} rows with negative quantity - REMOVING (returns/cancellations)\")\n",
    "    df = df[df['Quantity'] > 0]\n",
    "    decision = f\"NEGATIVE QUANTITY: Removed {negative_qty} negative quantity rows (cancelled orders)\"\n",
    "    decisions.append(decision)\n",
    "\n",
    "# Check for negative discount\n",
    "negative_discount = (df['Discount'] < 0).sum() if 'Discount' in df.columns else 0\n",
    "if negative_discount > 0:\n",
    "    print(f\"  Found {negative_discount} rows with negative discount - KEEPING (markup possible)\")\n",
    "    decision = f\"NEGATIVE DISCOUNT: Kept {negative_discount} negative discount rows (markup/special pricing)\"\n",
    "    decisions.append(decision)\n",
    "\n",
    "# ===== DECISION 6: Remove Outliers (Domain Logic) =====\n",
    "print(\"\\nüìä Checking for statistical outliers...\")\n",
    "\n",
    "# For quantity: anything > 20 units per order is unusual (typical retail)\n",
    "if 'Quantity' in df.columns:\n",
    "    Q1 = df['Quantity'].quantile(0.25)\n",
    "    Q3 = df['Quantity'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((df['Quantity'] < Q1 - 1.5*IQR) | (df['Quantity'] > Q3 + 1.5*IQR)).sum()\n",
    "    print(f\"  Quantity outliers (IQR method): {outliers} rows\")\n",
    "    # Keep them for now - let's see them in analysis\n",
    "    decision = f\"QUANTITY OUTLIERS: Kept {outliers} outliers (high-volume orders are valid)\"\n",
    "    decisions.append(decision)\n",
    "\n",
    "# ===== DECISION 7: Handle Invalid Dates =====\n",
    "invalid_dates = 0\n",
    "for col in df.columns:\n",
    "    if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "        invalid_count = df[col].isnull().sum()\n",
    "        if invalid_count > 0:\n",
    "            print(f\"  Invalid dates in {col}: {invalid_count}\")\n",
    "            invalid_dates += invalid_count\n",
    "            # Drop rows with invalid dates\n",
    "            df = df.dropna(subset=[col])\n",
    "            decision = f\"INVALID DATES in '{col}': Removed {invalid_count} rows\"\n",
    "            decisions.append(decision)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ===== FEATURE 1: Order Date Features =====\n",
    "print(\"\\nüî® Creating temporal features...\")\n",
    "if 'Order Date' in df.columns:\n",
    "    df['order_year'] = df['Order Date'].dt.year\n",
    "    df['order_month'] = df['Order Date'].dt.month\n",
    "    df['order_quarter'] = df['Order Date'].dt.quarter\n",
    "    df['order_day_of_week'] = df['Order Date'].dt.dayofweek\n",
    "    df['order_week_of_year'] = df['Order Date'].dt.isocalendar().week\n",
    "    print(\"‚úÖ Created: order_year, order_month, order_quarter, order_day_of_week, order_week_of_year\")\n",
    "\n",
    "# ===== FEATURE 2: Profit Metrics =====\n",
    "print(\"\\nüí∞ Creating profit features...\")\n",
    "if 'Sales' in df.columns and 'Profit' in df.columns:\n",
    "    df['profit_margin'] = (df['Profit'] / df['Sales'] * 100).round(2)\n",
    "    df['profit_margin'] = df['profit_margin'].replace([np.inf, -np.inf], 0)  # Handle division by zero\n",
    "    print(\"‚úÖ Created: profit_margin (%)\")\n",
    "\n",
    "# ===== FEATURE 3: Discount Flag =====\n",
    "print(\"\\nüè∑Ô∏è Creating discount features...\")\n",
    "if 'Discount' in df.columns:\n",
    "    df['has_discount'] = (df['Discount'] > 0).astype(int)\n",
    "    df['high_discount'] = (df['Discount'] > df['Discount'].median()).astype(int)\n",
    "    print(\"‚úÖ Created: has_discount, high_discount\")\n",
    "\n",
    "# ===== FEATURE 4: Customer Type (New vs Returning) =====\n",
    "print(\"\\nüë• Creating customer features...\")\n",
    "if 'Customer ID' in df.columns and 'Order Date' in df.columns:\n",
    "    # Find first order date for each customer\n",
    "    customer_first_order = df.groupby('Customer ID')['Order Date'].min().reset_index()\n",
    "    customer_first_order.columns = ['Customer ID', 'First Order Date']\n",
    "    \n",
    "    df = df.merge(customer_first_order, on='Customer ID', how='left')\n",
    "    \n",
    "    # Customer type: if this is their first order\n",
    "    df['customer_type'] = df.apply(\n",
    "        lambda row: 'New' if row['Order Date'] == row['First Order Date'] else 'Returning',\n",
    "        axis=1\n",
    "    )\n",
    "    print(\"‚úÖ Created: customer_type (New/Returning)\")\n",
    "\n",
    "# ===== FEATURE 5: Customer Order Frequency & AOV =====\n",
    "print(\"\\nüìä Creating customer aggregation features...\")\n",
    "if 'Customer ID' in df.columns:\n",
    "    customer_stats = df.groupby('Customer ID').agg({\n",
    "        'Order ID': 'count',  # Number of orders\n",
    "        'Sales': ['sum', 'mean'],  # Total sales & average order value\n",
    "        'Profit': 'sum',\n",
    "        'Order Date': 'min'  # First order date\n",
    "    }).reset_index()\n",
    "    \n",
    "    customer_stats.columns = ['Customer ID', 'order_frequency', 'total_customer_sales', \n",
    "                             'avg_order_value', 'total_customer_profit', 'customer_first_order']\n",
    "    \n",
    "    df = df.merge(customer_stats, on='Customer ID', how='left')\n",
    "    print(\"‚úÖ Created: order_frequency, avg_order_value (per customer)\")\n",
    "\n",
    "# ===== FEATURE 6: Delivery Delay Flag =====\n",
    "print(\"\\n‚è±Ô∏è Creating delivery features...\")\n",
    "if 'Ship Date' in df.columns and 'Order Date' in df.columns:\n",
    "    df['delivery_days'] = (df['Ship Date'] - df['Order Date']).dt.days\n",
    "    df['delivery_delay_flag'] = (df['delivery_days'] > df['delivery_days'].median()).astype(int)\n",
    "    print(\"‚úÖ Created: delivery_days, delivery_delay_flag\")\n",
    "\n",
    "# ===== FEATURE 7: Revenue Segment =====\n",
    "print(\"\\nüíé Creating customer value segmentation...\")\n",
    "if 'Sales' in df.columns:\n",
    "    df['revenue_segment'] = pd.qcut(df['Sales'], q=3, labels=['Low', 'Medium', 'High'], duplicates='drop')\n",
    "    print(\"‚úÖ Created: revenue_segment (Low/Medium/High)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n‚ú® Final Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nüéØ New Features Created:\")\n",
    "print(f\"  - Temporal: order_year, order_month, order_quarter, order_day_of_week, order_week_of_year\")\n",
    "print(f\"  - Financial: profit_margin, has_discount, high_discount, revenue_segment\")\n",
    "print(f\"  - Customer: customer_type, order_frequency, avg_order_value, total_customer_sales\")\n",
    "print(f\"  - Delivery: delivery_days, delivery_delay_flag\")\n",
    "\n",
    "print(f\"\\nüìã Decision Log:\")\n",
    "for i, decision in enumerate(decisions, 1):\n",
    "    print(f\"  {i}. {decision}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAVING CLEANED DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save cleaned data\n",
    "df.to_csv('../data/processed/superstore_cleaned.csv', index=False)\n",
    "print(\"\\n‚úÖ Cleaned data saved to: data/processed/superstore_cleaned.csv\")\n",
    "print(f\"   Total rows: {len(df)}\")\n",
    "print(f\"   Total columns: {len(df.columns)}\")\n",
    "\n",
    "# Display final dataframe\n",
    "print(f\"\\nüìä Sample of cleaned data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cb02b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify cleaned data\n",
    "print(df.info())\n",
    "print(\"\\n‚úÖ Data cleaning and feature engineering complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
