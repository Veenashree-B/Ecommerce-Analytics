{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1638eb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: LOAD & EXPLORE DATA\n",
      "================================================================================\n",
      "\n",
      "üìä Dataset Shape: (9994, 21)\n",
      "\n",
      "üìã Column Names & Types:\n",
      "Row ID             int64\n",
      "Order ID          object\n",
      "Order Date        object\n",
      "Ship Date         object\n",
      "Ship Mode         object\n",
      "Customer ID       object\n",
      "Customer Name     object\n",
      "Segment           object\n",
      "Country           object\n",
      "City              object\n",
      "State             object\n",
      "Postal Code        int64\n",
      "Region            object\n",
      "Product ID        object\n",
      "Category          object\n",
      "Sub-Category      object\n",
      "Product Name      object\n",
      "Sales            float64\n",
      "Quantity           int64\n",
      "Discount         float64\n",
      "Profit           float64\n",
      "dtype: object\n",
      "\n",
      "üîç First few rows:\n",
      "   Row ID        Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
      "0       1  CA-2016-152156   11/8/2016  11/11/2016    Second Class    CG-12520   \n",
      "1       2  CA-2016-152156   11/8/2016  11/11/2016    Second Class    CG-12520   \n",
      "2       3  CA-2016-138688   6/12/2016   6/16/2016    Second Class    DV-13045   \n",
      "3       4  US-2015-108966  10/11/2015  10/18/2015  Standard Class    SO-20335   \n",
      "4       5  US-2015-108966  10/11/2015  10/18/2015  Standard Class    SO-20335   \n",
      "\n",
      "     Customer Name    Segment        Country             City  ...  \\\n",
      "0      Claire Gute   Consumer  United States        Henderson  ...   \n",
      "1      Claire Gute   Consumer  United States        Henderson  ...   \n",
      "2  Darrin Van Huff  Corporate  United States      Los Angeles  ...   \n",
      "3   Sean O'Donnell   Consumer  United States  Fort Lauderdale  ...   \n",
      "4   Sean O'Donnell   Consumer  United States  Fort Lauderdale  ...   \n",
      "\n",
      "  Postal Code  Region       Product ID         Category Sub-Category  \\\n",
      "0       42420   South  FUR-BO-10001798        Furniture    Bookcases   \n",
      "1       42420   South  FUR-CH-10000454        Furniture       Chairs   \n",
      "2       90036    West  OFF-LA-10000240  Office Supplies       Labels   \n",
      "3       33311   South  FUR-TA-10000577        Furniture       Tables   \n",
      "4       33311   South  OFF-ST-10000760  Office Supplies      Storage   \n",
      "\n",
      "                                        Product Name     Sales  Quantity  \\\n",
      "0                  Bush Somerset Collection Bookcase  261.9600         2   \n",
      "1  Hon Deluxe Fabric Upholstered Stacking Chairs,...  731.9400         3   \n",
      "2  Self-Adhesive Address Labels for Typewriters b...   14.6200         2   \n",
      "3      Bretford CR4500 Series Slim Rectangular Table  957.5775         5   \n",
      "4                     Eldon Fold 'N Roll Cart System   22.3680         2   \n",
      "\n",
      "   Discount    Profit  \n",
      "0      0.00   41.9136  \n",
      "1      0.00  219.5820  \n",
      "2      0.00    6.8714  \n",
      "3      0.45 -383.0310  \n",
      "4      0.20    2.5164  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "üìà Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9994 entries, 0 to 9993\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Row ID         9994 non-null   int64  \n",
      " 1   Order ID       9994 non-null   object \n",
      " 2   Order Date     9994 non-null   object \n",
      " 3   Ship Date      9994 non-null   object \n",
      " 4   Ship Mode      9994 non-null   object \n",
      " 5   Customer ID    9994 non-null   object \n",
      " 6   Customer Name  9994 non-null   object \n",
      " 7   Segment        9994 non-null   object \n",
      " 8   Country        9994 non-null   object \n",
      " 9   City           9994 non-null   object \n",
      " 10  State          9994 non-null   object \n",
      " 11  Postal Code    9994 non-null   int64  \n",
      " 12  Region         9994 non-null   object \n",
      " 13  Product ID     9994 non-null   object \n",
      " 14  Category       9994 non-null   object \n",
      " 15  Sub-Category   9994 non-null   object \n",
      " 16  Product Name   9994 non-null   object \n",
      " 17  Sales          9994 non-null   float64\n",
      " 18  Quantity       9994 non-null   int64  \n",
      " 19  Discount       9994 non-null   float64\n",
      " 20  Profit         9994 non-null   float64\n",
      "dtypes: float64(3), int64(3), object(15)\n",
      "memory usage: 1.6+ MB\n",
      "None\n",
      "\n",
      "‚ùì Missing Values:\n",
      "No missing values\n",
      "\n",
      "================================================================================\n",
      "STEP 2: DATA CLEANING - DECISION LOG\n",
      "================================================================================\n",
      "\n",
      "‚úÖ MISSING VALUES: No missing values found in dataset\n",
      "‚úÖ DUPLICATES: Removed 0 duplicate rows\n",
      "\n",
      "üîÑ Parsing date columns...\n",
      "Detected date columns: ['Order ID', 'Order Date', 'Ship Date']\n",
      "‚úÖ DATE PARSING: Converted 'Order Date' to datetime\n",
      "‚úÖ DATE PARSING: Converted 'Ship Date' to datetime\n",
      "\n",
      "üè∑Ô∏è Checking categorical columns...\n",
      "  Order ID: 5009 unique values\n",
      "  Ship Mode: 4 unique values\n",
      "  Customer ID: 793 unique values\n",
      "  Customer Name: 793 unique values\n",
      "  Segment: 3 unique values\n",
      "  Country: 1 unique values\n",
      "  City: 531 unique values\n",
      "  State: 49 unique values\n",
      "  Region: 4 unique values\n",
      "  Product ID: 1862 unique values\n",
      "  Category: 3 unique values\n",
      "  Sub-Category: 17 unique values\n",
      "  Product Name: 1850 unique values\n",
      "‚úÖ CATEGORIES: Stripped whitespace from all categorical columns\n",
      "\n",
      "‚ö†Ô∏è Checking for invalid/negative values...\n",
      "  Found 1871 rows with negative profit (KEPT - some products DO lose money)\n",
      "\n",
      "üìä Checking for statistical outliers...\n",
      "  Quantity outliers (IQR method): 170 rows\n",
      "\n",
      "================================================================================\n",
      "STEP 3: FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "üî® Creating temporal features...\n",
      "‚úÖ Created: order_year, order_month, order_quarter, order_day_of_week, order_week_of_year\n",
      "\n",
      "üí∞ Creating profit features...\n",
      "‚úÖ Created: profit_margin (%)\n",
      "\n",
      "üè∑Ô∏è Creating discount features...\n",
      "‚úÖ Created: has_discount, high_discount\n",
      "\n",
      "üë• Creating customer features...\n",
      "‚úÖ Created: customer_type (New/Returning)\n",
      "\n",
      "üìä Creating customer aggregation features...\n",
      "‚úÖ Created: order_frequency, avg_order_value (per customer)\n",
      "\n",
      "‚è±Ô∏è Creating delivery features...\n",
      "‚úÖ Created: delivery_days, delivery_delay_flag\n",
      "\n",
      "üíé Creating customer value segmentation...\n",
      "‚úÖ Created: revenue_segment (Low/Medium/High)\n",
      "\n",
      "================================================================================\n",
      "DATA QUALITY SUMMARY\n",
      "================================================================================\n",
      "\n",
      "‚ú® Final Dataset Shape: (9994, 39)\n",
      "\n",
      "üéØ New Features Created:\n",
      "  - Temporal: order_year, order_month, order_quarter, order_day_of_week, order_week_of_year\n",
      "  - Financial: profit_margin, has_discount, high_discount, revenue_segment\n",
      "  - Customer: customer_type, order_frequency, avg_order_value, total_customer_sales\n",
      "  - Delivery: delivery_days, delivery_delay_flag\n",
      "\n",
      "üìã Decision Log:\n",
      "  1. MISSING VALUES: No missing values found in dataset\n",
      "  2. DUPLICATES: Removed 0 duplicate rows\n",
      "  3. DATE PARSING: Converted 'Order Date' to datetime\n",
      "  4. DATE PARSING: Converted 'Ship Date' to datetime\n",
      "  5. CATEGORIES: Stripped whitespace from all categorical columns\n",
      "  6. NEGATIVE PROFIT: Kept 1871 negative profit rows (domain logic: some orders lose money)\n",
      "  7. QUANTITY OUTLIERS: Kept 170 outliers (high-volume orders are valid)\n",
      "\n",
      "================================================================================\n",
      "SAVING CLEANED DATA\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Cleaned data saved to: data/processed/superstore_cleaned.csv\n",
      "   Total rows: 9994\n",
      "   Total columns: 39\n",
      "\n",
      "üìä Sample of cleaned data:\n",
      "   Row ID        Order ID Order Date  Ship Date       Ship Mode Customer ID  \\\n",
      "0       1  CA-2016-152156 2016-11-08 2016-11-11    Second Class    CG-12520   \n",
      "1       2  CA-2016-152156 2016-11-08 2016-11-11    Second Class    CG-12520   \n",
      "2       3  CA-2016-138688 2016-06-12 2016-06-16    Second Class    DV-13045   \n",
      "3       4  US-2015-108966 2015-10-11 2015-10-18  Standard Class    SO-20335   \n",
      "4       5  US-2015-108966 2015-10-11 2015-10-18  Standard Class    SO-20335   \n",
      "\n",
      "     Customer Name    Segment        Country             City  ...  \\\n",
      "0      Claire Gute   Consumer  United States        Henderson  ...   \n",
      "1      Claire Gute   Consumer  United States        Henderson  ...   \n",
      "2  Darrin Van Huff  Corporate  United States      Los Angeles  ...   \n",
      "3   Sean O'Donnell   Consumer  United States  Fort Lauderdale  ...   \n",
      "4   Sean O'Donnell   Consumer  United States  Fort Lauderdale  ...   \n",
      "\n",
      "  First Order Date  customer_type order_frequency total_customer_sales  \\\n",
      "0       2015-10-15      Returning               5            1148.7800   \n",
      "1       2015-10-15      Returning               5            1148.7800   \n",
      "2       2016-06-12            New               9            1119.4830   \n",
      "3       2015-10-11            New              15            2602.5755   \n",
      "4       2015-10-11            New              15            2602.5755   \n",
      "\n",
      "  avg_order_value total_customer_profit customer_first_order  delivery_days  \\\n",
      "0      229.756000              169.9344           2015-10-15              3   \n",
      "1      229.756000              169.9344           2015-10-15              3   \n",
      "2      124.387000             -427.1840           2016-06-12              4   \n",
      "3      173.505033              -81.0858           2015-10-11              7   \n",
      "4      173.505033              -81.0858           2015-10-11              7   \n",
      "\n",
      "   delivery_delay_flag  revenue_segment  \n",
      "0                    0             High  \n",
      "1                    0             High  \n",
      "2                    0              Low  \n",
      "3                    1             High  \n",
      "4                    1              Low  \n",
      "\n",
      "[5 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=================================================================\n",
    "PHASE 2 & 3: DATA CLEANING & FEATURE ENGINEERING\n",
    "=================================================================\n",
    "This notebook handles:\n",
    "1. Loading and exploring the raw data\n",
    "2. Cleaning & handling missing values (logical decisions documented)\n",
    "3. Removing duplicates & anomalies\n",
    "4. Creating engineered features for analysis\n",
    "=================================================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: LOAD & EXPLORE DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load data with proper encoding handling\n",
    "df = pd.read_csv('../data/raw/superstore.csv', encoding='latin-1')\n",
    "\n",
    "print(f\"\\nüìä Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nüìã Column Names & Types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nüîç First few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nüìà Data Info:\")\n",
    "print(df.info())\n",
    "print(f\"\\n‚ùì Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0] if missing.sum() > 0 else \"No missing values\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: DATA CLEANING - DECISION LOG\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a decision log\n",
    "decisions = []\n",
    "\n",
    "# ===== DECISION 1: Handle Missing Values =====\n",
    "decision = \"MISSING VALUES: No missing values found in dataset\"\n",
    "print(f\"\\n‚úÖ {decision}\")\n",
    "decisions.append(decision)\n",
    "\n",
    "# ===== DECISION 2: Remove Duplicates =====\n",
    "duplicates_before = df.duplicated().sum()\n",
    "df = df.drop_duplicates()\n",
    "duplicates_after = df.duplicated().sum()\n",
    "decision = f\"DUPLICATES: Removed {duplicates_before} duplicate rows\"\n",
    "print(f\"‚úÖ {decision}\")\n",
    "decisions.append(decision)\n",
    "\n",
    "# ===== DECISION 3: Parse Date Columns =====\n",
    "print(\"\\nüîÑ Parsing date columns...\")\n",
    "# Identify date columns (adjust based on your actual data)\n",
    "date_columns = [col for col in df.columns if 'date' in col.lower() or 'order' in col.lower()]\n",
    "print(f\"Detected date columns: {date_columns}\")\n",
    "\n",
    "# Parse dates\n",
    "for col in df.columns:\n",
    "    if 'date' in col.lower():\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        decision = f\"DATE PARSING: Converted '{col}' to datetime\"\n",
    "        print(f\"‚úÖ {decision}\")\n",
    "        decisions.append(decision)\n",
    "\n",
    "# ===== DECISION 4: Handle Inconsistent Categories =====\n",
    "print(\"\\nüè∑Ô∏è Checking categorical columns...\")\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "for col in categorical_cols:\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"  {col}: {unique_count} unique values\")\n",
    "    # Strip whitespace from all object columns\n",
    "    df[col] = df[col].str.strip() if df[col].dtype == 'object' else df[col]\n",
    "\n",
    "decision = \"CATEGORIES: Stripped whitespace from all categorical columns\"\n",
    "print(f\"‚úÖ {decision}\")\n",
    "decisions.append(decision)\n",
    "\n",
    "# ===== DECISION 5: Handle Negative/Invalid Values =====\n",
    "print(\"\\n‚ö†Ô∏è Checking for invalid/negative values...\")\n",
    "\n",
    "# Check for negative profit\n",
    "negative_profit = (df['Profit'] < 0).sum() if 'Profit' in df.columns else 0\n",
    "if negative_profit > 0:\n",
    "    print(f\"  Found {negative_profit} rows with negative profit (KEPT - some products DO lose money)\")\n",
    "    decision = f\"NEGATIVE PROFIT: Kept {negative_profit} negative profit rows (domain logic: some orders lose money)\"\n",
    "    decisions.append(decision)\n",
    "\n",
    "# Check for negative quantity\n",
    "negative_qty = (df['Quantity'] < 0).sum() if 'Quantity' in df.columns else 0\n",
    "if negative_qty > 0:\n",
    "    print(f\"  Found {negative_qty} rows with negative quantity - REMOVING (returns/cancellations)\")\n",
    "    df = df[df['Quantity'] > 0]\n",
    "    decision = f\"NEGATIVE QUANTITY: Removed {negative_qty} negative quantity rows (cancelled orders)\"\n",
    "    decisions.append(decision)\n",
    "\n",
    "# Check for negative discount\n",
    "negative_discount = (df['Discount'] < 0).sum() if 'Discount' in df.columns else 0\n",
    "if negative_discount > 0:\n",
    "    print(f\"  Found {negative_discount} rows with negative discount - KEEPING (markup possible)\")\n",
    "    decision = f\"NEGATIVE DISCOUNT: Kept {negative_discount} negative discount rows (markup/special pricing)\"\n",
    "    decisions.append(decision)\n",
    "\n",
    "# ===== DECISION 6: Remove Outliers (Domain Logic) =====\n",
    "print(\"\\nüìä Checking for statistical outliers...\")\n",
    "\n",
    "# For quantity: anything > 20 units per order is unusual (typical retail)\n",
    "if 'Quantity' in df.columns:\n",
    "    Q1 = df['Quantity'].quantile(0.25)\n",
    "    Q3 = df['Quantity'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((df['Quantity'] < Q1 - 1.5*IQR) | (df['Quantity'] > Q3 + 1.5*IQR)).sum()\n",
    "    print(f\"  Quantity outliers (IQR method): {outliers} rows\")\n",
    "    # Keep them for now - let's see them in analysis\n",
    "    decision = f\"QUANTITY OUTLIERS: Kept {outliers} outliers (high-volume orders are valid)\"\n",
    "    decisions.append(decision)\n",
    "\n",
    "# ===== DECISION 7: Handle Invalid Dates =====\n",
    "invalid_dates = 0\n",
    "for col in df.columns:\n",
    "    if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "        invalid_count = df[col].isnull().sum()\n",
    "        if invalid_count > 0:\n",
    "            print(f\"  Invalid dates in {col}: {invalid_count}\")\n",
    "            invalid_dates += invalid_count\n",
    "            # Drop rows with invalid dates\n",
    "            df = df.dropna(subset=[col])\n",
    "            decision = f\"INVALID DATES in '{col}': Removed {invalid_count} rows\"\n",
    "            decisions.append(decision)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ===== FEATURE 1: Order Date Features =====\n",
    "print(\"\\nüî® Creating temporal features...\")\n",
    "if 'Order Date' in df.columns:\n",
    "    df['order_year'] = df['Order Date'].dt.year\n",
    "    df['order_month'] = df['Order Date'].dt.month\n",
    "    df['order_quarter'] = df['Order Date'].dt.quarter\n",
    "    df['order_day_of_week'] = df['Order Date'].dt.dayofweek\n",
    "    df['order_week_of_year'] = df['Order Date'].dt.isocalendar().week\n",
    "    print(\"‚úÖ Created: order_year, order_month, order_quarter, order_day_of_week, order_week_of_year\")\n",
    "\n",
    "# ===== FEATURE 2: Profit Metrics =====\n",
    "print(\"\\nüí∞ Creating profit features...\")\n",
    "if 'Sales' in df.columns and 'Profit' in df.columns:\n",
    "    df['profit_margin'] = (df['Profit'] / df['Sales'] * 100).round(2)\n",
    "    df['profit_margin'] = df['profit_margin'].replace([np.inf, -np.inf], 0)  # Handle division by zero\n",
    "    print(\"‚úÖ Created: profit_margin (%)\")\n",
    "\n",
    "# ===== FEATURE 3: Discount Flag =====\n",
    "print(\"\\nüè∑Ô∏è Creating discount features...\")\n",
    "if 'Discount' in df.columns:\n",
    "    df['has_discount'] = (df['Discount'] > 0).astype(int)\n",
    "    df['high_discount'] = (df['Discount'] > df['Discount'].median()).astype(int)\n",
    "    print(\"‚úÖ Created: has_discount, high_discount\")\n",
    "\n",
    "# ===== FEATURE 4: Customer Type (New vs Returning) =====\n",
    "print(\"\\nüë• Creating customer features...\")\n",
    "if 'Customer ID' in df.columns and 'Order Date' in df.columns:\n",
    "    # Find first order date for each customer\n",
    "    customer_first_order = df.groupby('Customer ID')['Order Date'].min().reset_index()\n",
    "    customer_first_order.columns = ['Customer ID', 'First Order Date']\n",
    "    \n",
    "    df = df.merge(customer_first_order, on='Customer ID', how='left')\n",
    "    \n",
    "    # Customer type: if this is their first order\n",
    "    df['customer_type'] = df.apply(\n",
    "        lambda row: 'New' if row['Order Date'] == row['First Order Date'] else 'Returning',\n",
    "        axis=1\n",
    "    )\n",
    "    print(\"‚úÖ Created: customer_type (New/Returning)\")\n",
    "\n",
    "# ===== FEATURE 5: Customer Order Frequency & AOV =====\n",
    "print(\"\\nüìä Creating customer aggregation features...\")\n",
    "if 'Customer ID' in df.columns:\n",
    "    customer_stats = df.groupby('Customer ID').agg({\n",
    "        'Order ID': 'count',  # Number of orders\n",
    "        'Sales': ['sum', 'mean'],  # Total sales & average order value\n",
    "        'Profit': 'sum',\n",
    "        'Order Date': 'min'  # First order date\n",
    "    }).reset_index()\n",
    "    \n",
    "    customer_stats.columns = ['Customer ID', 'order_frequency', 'total_customer_sales', \n",
    "                             'avg_order_value', 'total_customer_profit', 'customer_first_order']\n",
    "    \n",
    "    df = df.merge(customer_stats, on='Customer ID', how='left')\n",
    "    print(\"‚úÖ Created: order_frequency, avg_order_value (per customer)\")\n",
    "\n",
    "# ===== FEATURE 6: Delivery Delay Flag =====\n",
    "print(\"\\n‚è±Ô∏è Creating delivery features...\")\n",
    "if 'Ship Date' in df.columns and 'Order Date' in df.columns:\n",
    "    df['delivery_days'] = (df['Ship Date'] - df['Order Date']).dt.days\n",
    "    df['delivery_delay_flag'] = (df['delivery_days'] > df['delivery_days'].median()).astype(int)\n",
    "    print(\"‚úÖ Created: delivery_days, delivery_delay_flag\")\n",
    "\n",
    "# ===== FEATURE 7: Revenue Segment =====\n",
    "print(\"\\nüíé Creating customer value segmentation...\")\n",
    "if 'Sales' in df.columns:\n",
    "    df['revenue_segment'] = pd.qcut(df['Sales'], q=3, labels=['Low', 'Medium', 'High'], duplicates='drop')\n",
    "    print(\"‚úÖ Created: revenue_segment (Low/Medium/High)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n‚ú® Final Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nüéØ New Features Created:\")\n",
    "print(f\"  - Temporal: order_year, order_month, order_quarter, order_day_of_week, order_week_of_year\")\n",
    "print(f\"  - Financial: profit_margin, has_discount, high_discount, revenue_segment\")\n",
    "print(f\"  - Customer: customer_type, order_frequency, avg_order_value, total_customer_sales\")\n",
    "print(f\"  - Delivery: delivery_days, delivery_delay_flag\")\n",
    "\n",
    "print(f\"\\nüìã Decision Log:\")\n",
    "for i, decision in enumerate(decisions, 1):\n",
    "    print(f\"  {i}. {decision}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAVING CLEANED DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save cleaned data\n",
    "df.to_csv('../data/processed/superstore_cleaned.csv', index=False)\n",
    "print(\"\\n‚úÖ Cleaned data saved to: data/processed/superstore_cleaned.csv\")\n",
    "print(f\"   Total rows: {len(df)}\")\n",
    "print(f\"   Total columns: {len(df.columns)}\")\n",
    "\n",
    "# Display final dataframe\n",
    "print(f\"\\nüìä Sample of cleaned data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cb02b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9994 entries, 0 to 9993\n",
      "Data columns (total 39 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   Row ID                 9994 non-null   int64         \n",
      " 1   Order ID               9994 non-null   object        \n",
      " 2   Order Date             9994 non-null   datetime64[ns]\n",
      " 3   Ship Date              9994 non-null   datetime64[ns]\n",
      " 4   Ship Mode              9994 non-null   object        \n",
      " 5   Customer ID            9994 non-null   object        \n",
      " 6   Customer Name          9994 non-null   object        \n",
      " 7   Segment                9994 non-null   object        \n",
      " 8   Country                9994 non-null   object        \n",
      " 9   City                   9994 non-null   object        \n",
      " 10  State                  9994 non-null   object        \n",
      " 11  Postal Code            9994 non-null   int64         \n",
      " 12  Region                 9994 non-null   object        \n",
      " 13  Product ID             9994 non-null   object        \n",
      " 14  Category               9994 non-null   object        \n",
      " 15  Sub-Category           9994 non-null   object        \n",
      " 16  Product Name           9994 non-null   object        \n",
      " 17  Sales                  9994 non-null   float64       \n",
      " 18  Quantity               9994 non-null   int64         \n",
      " 19  Discount               9994 non-null   float64       \n",
      " 20  Profit                 9994 non-null   float64       \n",
      " 21  order_year             9994 non-null   int32         \n",
      " 22  order_month            9994 non-null   int32         \n",
      " 23  order_quarter          9994 non-null   int32         \n",
      " 24  order_day_of_week      9994 non-null   int32         \n",
      " 25  order_week_of_year     9994 non-null   UInt32        \n",
      " 26  profit_margin          9994 non-null   float64       \n",
      " 27  has_discount           9994 non-null   int32         \n",
      " 28  high_discount          9994 non-null   int32         \n",
      " 29  First Order Date       9994 non-null   datetime64[ns]\n",
      " 30  customer_type          9994 non-null   object        \n",
      " 31  order_frequency        9994 non-null   int64         \n",
      " 32  total_customer_sales   9994 non-null   float64       \n",
      " 33  avg_order_value        9994 non-null   float64       \n",
      " 34  total_customer_profit  9994 non-null   float64       \n",
      " 35  customer_first_order   9994 non-null   datetime64[ns]\n",
      " 36  delivery_days          9994 non-null   int64         \n",
      " 37  delivery_delay_flag    9994 non-null   int32         \n",
      " 38  revenue_segment        9994 non-null   category      \n",
      "dtypes: UInt32(1), category(1), datetime64[ns](4), float64(7), int32(7), int64(5), object(14)\n",
      "memory usage: 2.6+ MB\n",
      "None\n",
      "\n",
      "‚úÖ Data cleaning and feature engineering complete!\n"
     ]
    }
   ],
   "source": [
    "# Verify cleaned data\n",
    "print(df.info())\n",
    "print(\"\\n‚úÖ Data cleaning and feature engineering complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
